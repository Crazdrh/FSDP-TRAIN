# FSDP-TRAIN

Run Command:  torchrun --nproc_per_node=4 fsdp1.py  --experiment-name Chnage to your   --dataset-name chnage to your hf dataset    --model-name hf model name     --save-dir where saved    --cpu-offload on
FSDP Llama Fine‑Tuning Pipeline
This repository contains a fully‑offline pipeline for preparing text data, downloading a base Llama model, and fine‑tuning it with PyTorch 2 Fully Sharded Data Parallel (FSDP) using a single command. The main entry‑point is fsdp1.py, which implements a ZeRO‑3–equivalent training loop with sharded checkpoints, automatic resumption, gradient checkpointing and detailed memory/timing metrics. 

Table of Contents

1. Project Overview
2. Repository Layout
3. Quick Start
4. Prerequisites & Installation
5. Data Preparation
6. Model Acquisition
7. Distributed Fine‑Tuning with fsdp1.py
8. Checkpoint Format & Resumption
9. Monitoring & Logging
10. Tips & Troubleshooting
11. Frequently Asked Questions
12. Contributing
13. License
14. Acknowledgments

Project Overview

Large‑language‑model (LLM) training is memory‑intensive. Fully Sharded Data Parallel (FSDP) breaks model parameters, gradients and optimizer states into non‑overlapping shards that are distributed across GPUs, reducing the per‑GPU memory footprint to roughly 1 ⁄ N of ZeRO‑1 and enabling full‑precision fine‑tuning on commodity hardware. 

This repository provides:

Script

        Purpose
        
        Key Technologies
        
        ORGANIZE_DATA.py fileciteturn0file3
        
        Evenly shuffles raw files into split_* sub‑folders for parallel preprocessing
        
        shutil, Python I/O
        
        DATA-HF.py fileciteturn0file0
        
        Converts TXT/JSON/YAML/… files into a Hugging Face Dataset and saves Arrow shards for streaming
        
        datasets, optional PyYAML
        
        MODEL_DOWNLOAD.py fileciteturn0file2
        
        Offline download of model config, tokenizer and weights to $HF_HOME
        
        transformers, torch.device("meta")
        
        fsdp1.py fileciteturn0file1
        
        Multi‑GPU fine‑tuning loop with FSDP, checkpointing, resume support and detailed telemetry
        
        PyTorch 2.3 FSDP, torchrun, transformers
        
        While all helper scripts are covered, the remainder of this README deep‑dives into fsdp1.py—the heart of the training pipeline.

Repository Layout

├── data/                 # optional raw text files (any structure)
├── splits/               # generated by ORGANIZE_DATA.py (20 equal sub‑dirs)
├── hf_datasets/          # Arrow datasets produced by DATA‑HF.py
├── .cache/               # model & tokenizer cache (HF_HOME)
├── outputs/
│   └── <experiment>/     # sharded checkpoints, state.json, LR scheduler, logs
├── fsdp1.py
├── ORGANIZE_DATA.py
├── DATA-HF.py
├── MODEL_DOWNLOAD.py
└── README.md             # ← you are here

Quick Start

# 1️⃣ Install dependencies (see below for details)
$ conda env create -f environment.yml && conda activate llama-fsdp

# 2️⃣ Prepare dataset
$ python ORGANIZE_DATA.py --src_dir raw_txt --dst_root splits
$ python DATA-HF.py --data_folder splits --save_dir hf_datasets/my_corpus

# 3️⃣ Download base model offline
$ python MODEL_DOWNLOAD.py  \
    --model-name meta-llama/Llama-3.2-3B-Instruct

# 4️⃣ Launch distributed fine‑tuning on 4 GPUs
$ torchrun --nnodes 1 --nproc_per_node 4 fsdp1.py \
    -e llama3-finetune \
    -d hf_datasets/my_corpus \
    -m meta-llama/Llama-3.2-3B-Instruct \
    --save-dir outputs \
    --batch-size 2 --lr 2e-5 --num-epochs 3 \
    --cpu-offload on

After each ckpt_freq steps a sharded checkpoint is written to outputs/llama3-finetune/. Restarting the same torchrun command will resume automatically.

Prerequisites & Installation

Hardware

GPUs ≥ 1 with CUDA 11.8+ (BF16 recommended; FP16 also works)

CPU RAM ≥ 32 GB (for data loading & tokenizer caches)

SSD storage for dataset shards and checkpoints

Software

Package

Tested Version

Python

 3.10 

 

PyTorch

 >= 2.3.0

 

CUDA Toolkit

 >= 11.8

torchvision

optional (metrics)

Hugging Face transformers

4.41

datasets

≥ 3.9

tqdm, pyyaml, argparse

standard

Install everything via the provided environment.yml or run pip install -r requirements.txt.

Data Preparation

1. Organize raw files

ORGANIZE_DATA.py creates split_00/ … split_19/ sub‑folders with equal numbers of text‑like files. This balances I/O when preprocessing in parallel.

$ python ORGANIZE_DATA.py \
    --src_dir ./raw_text \
    --dst_root ./splits --num_dirs 20

2. Convert to Hugging Face dataset

DATA-HF.py crawls each folder, extracts all string values from:

Plain‑text *.txt

Structured *.json (recursively traverses dictionaries & lists)

YAML *.yml / *.yaml (if PyYAML available)

and emits a Dataset with a single text column saved via dataset.save_to_disk() so it can be streamed without full RAM load.

$ python DATA-HF.py \
    --data_folder ./splits \
    --save_dir hf_datasets/my_corpus

The resulting Arrow shards can be consumed locally; no internet API calls are required.

Model Acquisition

To keep training fully offline we pre‑download both the configuration and weight shards using MODEL_DOWNLOAD.py.

$ export HF_HOME=$PWD/.cache/huggingface
$ python MODEL_DOWNLOAD.py \
    --model-name meta-llama/Llama-3.2-3B-Instruct

Internally the script loads the weights on a torch.device("meta") so nothing is actually allocated in GPU RAM during the download.

Distributed Fine‑Tuning with fsdp1.py

fsdp1.py is a self‑contained training script engineered for clarity—they remove external logging frameworks and rely solely on torchrun for process spawning and PyTorch native APIs. Below is a detailed tour of its architecture.

Launch Anatomy

torchrun \
  --standalone                      # or provide --nnodes / --node_rank / --rdzv_backend etc.
  --nproc_per_node 4                # GPUs per node
  fsdp1.py                          # ← main script
  -e  llama3-math-instruct          # experiment name (sub‑folder under save‑dir)
  -d  hf_datasets/my_corpus         # dataset name **or** path produced by DATA‑HF.py
  -m  meta-llama/Llama-3.2-3B-Instruct
  --save-dir outputs                # root for checkpoints/logs
  --batch-size 2
  --seq-length 1024
  --num-epochs 3
  --lr 2e-5
  --cpu-offload on                  # offload param shards to CPU between steps

Argument Reference

Flag

Default

Description

-e, --experiment-name

required

Folder name under save-dir used for checkpoints

-d, --dataset-name

required

Either a local path from DATA‑HF or a public HF dataset id (remote)

-m, --model-name

required

Any AutoModelForCausalLM‑compatible repo; weights must exist in $HF_HOME

--save-dir

../outputs

Root output directory (will be created)

--seed

 0

Global RNG seed (torch / cuda)

--num-epochs

6

Full‑dataset epochs

--lr

 3e-5

AdamW base learning rate

-b, --batch-size

 1

Per‑process batch size

--log-freq

 100

Steps between progress logs

--ckpt-freq

 15

Steps between sharded checkpoint saves

--seq-length

 1024

Context length after tokenization (overrides tokenizer max ≤ model limit)

--cpu-offload

 on

When on param shards are offloaded to host RAM after optimizer update

Internal Flow

Process Group Initialisation

dist.init_process_group()
rank = dist.get_rank(); local_rank = rank % torch.cuda.device_count()
torch.cuda.set_device(local_rank)

FSDP requires a DDP‑compatible process group. Using torchrun this is created automatically.

Model Construction

Rank 0 loads full weights on CPU so the exact parameter count is printed—every other rank spins up an empty meta model and populates weights during FSDP wrapping.

LlamaRMSNorm & LlamaRotaryEmbedding miss reset_parameters(), so dummy lambdas are monkey‑patched to satisfy FSDP’s param‑init API.

FSDP Wrapping

wrap_policy = partial(transformer_auto_wrap_policy,
                      transformer_layer_cls={LlamaDecoderLayer, Embedding})
model = FullyShardedDataParallel(
    model,
    device_id=local_rank,
    param_init_fn=lambda m: m.to_empty(device=device, recurse=False),
    sync_module_states=True,
    auto_wrap_policy=wrap_policy,
    sharding_strategy=ShardingStrategy.FULL_SHARD,  # ZeRO‑3 equivalent
    cpu_offload=CPUOffload(offload_params=args.cpu_offload == "on")
)

param_init_fn relocates freshly initialised tensors to an empty tensor on the target GPU, so no GPU OOM occurs before sharding.

sync_module_states=True ensures that the initial weights broadcast from rank 0.

auto_wrap_policy recursively wraps LlamaDecoderLayer & embedding tables only, minimising communication overhead.

Gradient Checkpointing — Memory vs Compute trade‑off

apply_activation_checkpointing(model,
    checkpoint_wrapper_fn=checkpoint_wrapper,
    auto_wrap_policy=wrap_policy)

Activations of each decoder block are recomputed during the backward pass, reducing peak memory by ~30 %.

Data Loading & Tokenisation

If dataset-name is a path, load_from_disk() is used—no internet traffic.

Tokenisation + grouping happen lazily via HF map with multi‑processing equal to multiprocessing.cpu_count().

The resulting dataset is sharded across ranks with DistributedSampler ensuring each GPU sees a unique subset each epoch.

Training Loop
The loop tracks four timers (data, forward, backward, update) capturing average milliseconds and reporting them every log_freq steps.

Timer

Scope

data

Token transfer CPU→GPU

forward

model(**batch)

backward

loss.backward() (all‑reduce embedded in FSDP)

update

optimizer.step() + LR scheduler

Checkpointing & Resume

Sharded state is saved via torch.distributed.checkpoint.save()—each rank writes its shard locally, so no single machine aggregates > GPU RAM.

An auxiliary state.json (epoch, global_step, etc.) and lr_scheduler.pt are stored by rank 0 only.

Upon startup, if these files exist they are loaded with torch.distributed.checkpoint.load() and training resumes seamlessly.

Memory Telemetryget_mem_stats() exposes current and peak allocation & reservation per process, printed in the log for quick diagnostics.

Example Log Snippet

[rank=0] total_mem_in_gb:40   curr_alloc:1.9   peak_alloc:14.9
[rank=0] {'global_step': 150, 'running_loss': 1.72, 'tok/s': 6404,
          'time/data': 7.9, 'time/forward': 34.1, 'time/backward': 55.4,
          'time/update': 12.2, 'time/total': 109.6}

Checkpoint Format & Resumption

outputs/
└── llama3-finetune/
    ├── checkpoint/            # per‑rank shard files (~1/GPUs of full model)
    ├── lr_scheduler.pt        # Pickled LR scheduler state
    └── state.json             # Epoch / step counters for auto‑resume

To resume: rerun the exact same torchrun command; the script discovers the directory and loads the shards. You may safely increase --num-epochs or change --log-freq / --ckpt-freq between runs—these are not stored in state.json.

Monitoring & Logging

Stdout/Stderr are timestamped and include the global rank.

Peak GPU memory is reset after each logging event so that the next window shows worst‑case consumption for that interval.

For more granular insights you can attach NVIDIA Nsight or nvidia-smi dmon because parameter sharding reduces resident memory to peaks of context length × hidden size.

Tips & Troubleshooting

Symptom

Possible Cause

Fix

CUDA out of memory at first forward

--batch-size too high, seq-length > model limit

Lower batch or enable --cpu-offload on

Hang after INFO Creating experiment root directory

Shared file‑system latency

Use node‑local SSD or set $TORCH_DISTRIBUTED_DEBUG=INFO

Validation required when using private HF model

Offline training; token not found

huggingface-cli login --token … on a machine with internet, then copy .cache directory

Checkpoint size larger than expected

cpu-offload off keeps FP32 optimizer states on GPU, doubling shard

Turn on CPU offload or switch to 8‑bit Adam

Frequently Asked Questions

Contributing

Fork the repo & create your feature branch (git checkout -b feature/foo)

Commit your changes (git commit -am 'Add awesome feature')

Push to the branch (git push origin feature/foo)

Open a Pull Request ✨

All contributions must pass flake8 and black (120‑char lines) as well as the minimal smoke test:

pytest tests/test_smoke.py

License

This project is licensed under the MIT License.

Acknowledgments

PyTorch FSDP team for upstream examples

Hugging Face transformers & datasets

The original Meta Llama‑3 authors

