# FSDP-TRAIN
FSDP Fine‑Tuning Pipeline
This repository contains a fully‑offline pipeline for preparing text data, downloading a base Hugging face model, and fine‑tuning it with PyTorch 2 Fully Sharded Data Parallel (FSDP) using a single command. The main entry‑point is fsdp1.py, which implements a ZeRO‑3–equivalent training loop with sharded checkpoints, automatic resumption, gradient checkpointing and detailed memory/timing metrics. 

Table of Contents:
1. Project Overview
2. Repository Layout
3. Quick Start
4. Prerequisites & Installation
5. Data Preparation
6. Model Acquisition
7. Distributed Fine‑Tuning with fsdp1.py
8. Checkpoint Format & Resumption
9. Monitoring & Logging
10. Tips & Troubleshooting
11. Frequently Asked Questions
12. Contributing
13. License
14. Acknowledgments

Project Overview
Large‑language‑model (LLM) training is memory‑intensive. Fully Sharded Data Parallel (FSDP) breaks model parameters, gradients, and optimizer states into non‑overlapping shards that are distributed evenly across GPUs, reducing the per‑GPU memory footprint to roughly 1 ⁄ N of ZeRO‑1 and enabling full‑precision fine‑tuning on commodity hardware. 

Repository Layout:

                ├── data/                 # optional raw text files (any structure)
                ├── splits/               # generated by ORGANIZE_DATA.py (20 equal sub‑dirs)
                ├── hf_datasets/          # Arrow datasets produced by DATA‑HF.py
                ├── .cache/               # model & tokenizer cache (HF_HOME)
                ├── outputs/
                │   └── <experiment>/     # sharded checkpoints, state.json, LR scheduler, logs
                ├── fsdp1.py
                ├── ORGANIZE_DATA.py
                ├── DATA-HF.py
                ├── MODEL_DOWNLOAD.py
                └── README.md      

After each ckpt_freq steps a sharded checkpoint is written to outputs dir. Restarting the same torchrun command will resume automatically.

Prerequisites & Installation:

        Hardware:
                GPUs ≥ 1 with CUDA 11.8+ (BF16 recommended, FP16 also works, FP32 also works)
                CPU RAM ≥ 32 GB (for data loading & tokenizer caches)
                SSD storage for dataset shards and checkpoint shards
        
        Software:
                Python => 3.10
                PyTorch >= 2.3.0
                CUDA Toolkit >= 11.8
                torchvision = optional (metrics)
                Hugging Face transformers = latest
                datasets = latest
                tqdm, pyyaml, argparse

Data Preparation:

    1. Organize raw files:
            ORGANIZE_DATA.py creates split_00/ … split_19/ sub‑folders with equal numbers of text‑like files. This balances I/O when preprocessing in parallel.
    
                $ python ORGANIZE_DATA.py \
                    --src_dir ./raw_text \
                    --dst_root ./splits --num_dirs 20
    
    2. Convert to Hugging Face dataset:
            DATA-HF.py crawls each folder, extracts all string values from:
    
            Plain‑text *.txt
       
            Structured *.json (recursively traverses dictionaries & lists)
       
            YAML *.yml / *.yaml (if PyYAML available)
       
            and emits a Dataset with a single text column saved via dataset.save_to_disk() so it can be streamed without full RAM load.
       
            $ python DATA-HF.py \
                --data_folder ./splits \
                --save_dir hf_datasets/my_corpus
The resulting Arrow shards can be consumed locally; no internet API calls are required.


Model Acquisition:
        
        To keep training fully offline we pre‑download both the configuration and weight shards using MODEL_DOWNLOAD.py.
        
        $ export HF_HOME=$PWD/.cache/huggingface
        $ python MODEL_DOWNLOAD.py \
            --model-name meta-llama/Llama-3.2-3B-Instruct
        
        Internally the script loads the weights on a torch.device("meta") so nothing is actually allocated in GPU RAM during the download.

Distributed Fine‑Tuning with fsdp1.py:

    fsdp1.py is a self‑contained training script engineered for clarity—they remove external logging frameworks and rely solely on torchrun for process spawning and PyTorch native APIs. Below is a detailed tour of its architecture.

    Launch Anatomy:
            
            torchrun \
              --standalone                      # or provide --nnodes / --node_rank / --rdzv_backend etc.
              --nproc_per_node 4                # GPUs per node
              fsdp1.py                          # ← main script
              -e  llama3-math-instruct          # experiment name (sub‑folder under save‑dir)
              -d  hf_datasets/my_corpus         # dataset name **or** path produced by DATA‑HF.py
              -m  meta-llama/Llama-3.2-3B-Instruct
              --save-dir outputs                # root for checkpoints/logs
              --batch-size 2
              --seq-length 1024
              --num-epochs 3
              --lr 2e-5
              --cpu-offload on                  # offload param shards to CPU between steps
    
    Argument Reference:
            
            torchrun == fsdp deafult run command to initialize fsdp
            --nproc_per_node "number of gpus you want to use(examppe is --nproc_per_node 4)"
            --experiment-name "This is where you will name your training run(it will save under this name)"   #Folder name under save-dir used for checkpoints
            --dataset-name "either a .arrow file path or hf dataset"    #Either a local path from DATA‑HF or a public HF dataset id (remote)(.arrow)
            --model-name "hf model name even with sharded checkpoints(original hf model)"    #Any AutoModelForCausalLM‑compatible repo; weights must exist in $HF_HOME
            --save-dir ../outputs    #Root output directory (will be created)
            --seed 0    #Global RNG seed (torch / cuda)
            --num-epochs 6    #Full epochs
            --lr 3e-5    #AdamW base learning rate
            -b, --batch-size 1    #Per‑process batch size
            --log-freq 100    #Steps between progress logs
            --ckpt-freq 15    #Steps between sharded checkpoint saves
            --seq-length 1024    #Context length after tokenization (overrides tokenizer max ≤ model limit)
            --cpu-offload on    #When on param shards are offloaded to host RAM after optimizer update



Internal Flow:

    Process Group Initialisation:
    
        dist.init_process_group()
        rank = dist.get_rank(); local_rank = rank % torch.cuda.device_count()
        torch.cuda.set_device(local_rank)
    
    FSDP requires a DDP‑compatible process group. Using torchrun this is created automatically.

Model Construction

Rank 0 loads full weights on CPU so the exact parameter count is printed—every other rank spins up an empty meta model and populates weights during FSDP wrapping.

LlamaRMSNorm & LlamaRotaryEmbedding miss reset_parameters(), so dummy lambdas are monkey‑patched to satisfy FSDP’s param‑init API.

FSDP Wrapping

wrap_policy = partial(transformer_auto_wrap_policy,
                      transformer_layer_cls={LlamaDecoderLayer, Embedding})
model = FullyShardedDataParallel(
    model,
    device_id=local_rank,
    param_init_fn=lambda m: m.to_empty(device=device, recurse=False),
    sync_module_states=True,
    auto_wrap_policy=wrap_policy,
    sharding_strategy=ShardingStrategy.FULL_SHARD,  # ZeRO‑3 equivalent
    cpu_offload=CPUOffload(offload_params=args.cpu_offload == "on")
)

param_init_fn relocates freshly initialised tensors to an empty tensor on the target GPU, so no GPU OOM occurs before sharding.

sync_module_states=True ensures that the initial weights broadcast from rank 0.

auto_wrap_policy recursively wraps LlamaDecoderLayer & embedding tables only, minimising communication overhead.

Gradient Checkpointing — Memory vs Compute trade‑off

apply_activation_checkpointing(model,
    checkpoint_wrapper_fn=checkpoint_wrapper,
    auto_wrap_policy=wrap_policy)

Activations of each decoder block are recomputed during the backward pass, reducing peak memory by ~30 %.

Data Loading & Tokenisation

If dataset-name is a path, load_from_disk() is used—no internet traffic.

Tokenisation + grouping happen lazily via HF map with multi‑processing equal to multiprocessing.cpu_count().

The resulting dataset is sharded across ranks with DistributedSampler ensuring each GPU sees a unique subset each epoch.

Training Loop
The loop tracks four timers (data, forward, backward, update) capturing average milliseconds and reporting them every log_freq steps.

Timer

Scope

data

Token transfer CPU→GPU

forward

model(**batch)

backward

loss.backward() (all‑reduce embedded in FSDP)

update

optimizer.step() + LR scheduler

Checkpointing & Resume

Sharded state is saved via torch.distributed.checkpoint.save()—each rank writes its shard locally, so no single machine aggregates > GPU RAM.

An auxiliary state.json (epoch, global_step, etc.) and lr_scheduler.pt are stored by rank 0 only.

Upon startup, if these files exist they are loaded with torch.distributed.checkpoint.load() and training resumes seamlessly.

Memory Telemetryget_mem_stats() exposes current and peak allocation & reservation per process, printed in the log for quick diagnostics.

Example Log Snippet

[rank=0] total_mem_in_gb:40   curr_alloc:1.9   peak_alloc:14.9
[rank=0] {'global_step': 150, 'running_loss': 1.72, 'tok/s': 6404,
          'time/data': 7.9, 'time/forward': 34.1, 'time/backward': 55.4,
          'time/update': 12.2, 'time/total': 109.6}

Checkpoint Format & Resumption

outputs/
└── llama3-finetune/
    ├── checkpoint/            # per‑rank shard files (~1/GPUs of full model)
    ├── lr_scheduler.pt        # Pickled LR scheduler state
    └── state.json             # Epoch / step counters for auto‑resume

To resume: rerun the exact same torchrun command; the script discovers the directory and loads the shards. You may safely increase --num-epochs or change --log-freq / --ckpt-freq between runs—these are not stored in state.json.

Monitoring & Logging

Stdout/Stderr are timestamped and include the global rank.

Peak GPU memory is reset after each logging event so that the next window shows worst‑case consumption for that interval.

For more granular insights you can attach NVIDIA Nsight or nvidia-smi dmon because parameter sharding reduces resident memory to peaks of context length × hidden size.

Tips & Troubleshooting

Symptom

Possible Cause

Fix

CUDA out of memory at first forward

--batch-size too high, seq-length > model limit

Lower batch or enable --cpu-offload on

Hang after INFO Creating experiment root directory

Shared file‑system latency

Use node‑local SSD or set $TORCH_DISTRIBUTED_DEBUG=INFO

Validation required when using private HF model

Offline training; token not found

huggingface-cli login --token … on a machine with internet, then copy .cache directory

Checkpoint size larger than expected

cpu-offload off keeps FP32 optimizer states on GPU, doubling shard

Turn on CPU offload or switch to 8‑bit Adam

Frequently Asked Questions

Contributing

Fork the repo & create your feature branch (git checkout -b feature/foo)

Commit your changes (git commit -am 'Add awesome feature')

Push to the branch (git push origin feature/foo)

Open a Pull Request ✨

All contributions must pass flake8 and black (120‑char lines) as well as the minimal smoke test:

pytest tests/test_smoke.py

License

This project is licensed under the MIT License.

Acknowledgments

PyTorch FSDP team for upstream examples

Hugging Face transformers & datasets

The original Meta Llama‑3 authors

